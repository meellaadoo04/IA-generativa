{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing conversation history with the ChatGPT model\n",
    "This sample notebook demonstrates a couple of simple patterns you can use for managing the prompts and conversation history with the ChatGPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\alumno_ai\\appdata\\roaming\\python\\python312\\site-packages (1.60.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\alumno_ai\\appdata\\roaming\\python\\python312\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os module & the OpenAI Python library for calling the OpenAI API\n",
    "# please make sure you have installed required libraries via pip install -r requirements.txt\n",
    "import os, dotenv\n",
    "import openai\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI  # Make sure you're using the correct import\n",
    "\n",
    "# Configuration settings (only need to set these once)\n",
    "deployment_name = \"gpt-4o-mini\"\n",
    "azure_endpoint = os.environ['AZURE_OPENAI_ENDPOINT']  # Make sure these environment variables are set\n",
    "api_key = os.environ['AZURE_OPENAI_API_KEY']\n",
    "api_version = \"2024-07-01-preview\"  # Verify this is the correct API version for your resource\n",
    "\n",
    "# Create the client instance (only need to do this once)\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=api_key,\n",
    "    api_version=api_version\n",
    ")\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Create the system message for ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant.\n"
     ]
    }
   ],
   "source": [
    "base_system_message = \"You are a helpful assistant.\"\n",
    "\n",
    "system_message = f\"{base_system_message.strip()}\"\n",
    "print(system_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Define helper functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in c:\\users\\alumno_ai\\appdata\\roaming\\python\\python312\\site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    encoding = tiktoken.encoding_for_model(model) # use the same model as the one used for the messages\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":  # if there's a name, the role is omitted\n",
    "                num_tokens += -1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to send the prompt to the ChatGPT model\n",
    "# More info : https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions\n",
    "def send_message(messages, model_name, max_response_tokens=500,temperature=0.0):\n",
    "    response = client.chat.completions.create(\n",
    "          model=model_name,\n",
    "          messages = messages,\n",
    "          max_tokens=max_response_tokens,\n",
    "          temperature=temperature\n",
    "                   \n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Defining a function to print out the conversation in a readable format\n",
    "def print_conversation(messages):\n",
    "    for message in messages:\n",
    "        print(f\"[{message['role'].upper()}]\")\n",
    "        print(message['content'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Start the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the first user message that will be sent to the model. Feel free to update this.\n",
    "user_message = \"I want to write a blog post about the impact of AI on the future of work.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of messages. role can be either \"user\" or \"assistant\" \n",
    "messages= [{\"role\": \"user\", \"content\": user_message}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 25\n"
     ]
    }
   ],
   "source": [
    "token_count = num_tokens_from_messages(messages)\n",
    "print(f\"Token count: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]\n",
      "I want to write a blog post about the impact of AI on the future of work.\n",
      "\n",
      "[ASSISTANT]\n",
      "### The Impact of AI on the Future of Work\n",
      "\n",
      "As we stand on the brink of a technological revolution, the impact of artificial intelligence (AI) on the future of work is becoming increasingly apparent. From reshaping industries to changing job roles and even redefining the skills needed in the workforce, AI is set to transform the way we work in profound ways.\n",
      "\n",
      "#### The Automation of Routine Tasks\n",
      "\n",
      "One of the most significant impacts of AI in the workplace is the automation of routine and repetitive tasks. Many\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_response_tokens = 100\n",
    "\n",
    "response = send_message(messages, deployment_name, max_response_tokens)\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "print_conversation(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Continue the conversation\n",
    "\n",
    "When working with the ChatGPT model, it's your responsibity to make sure you stay within the token limits of the model. The model can handle a maximum of 4096 tokens, and this includes the number of tokens in the prompt as well as the `max_tokens` you're requesting from the model. If you exceed these limits, the model will return an error.\n",
    "\n",
    "You should also consider the trade-off between maintaining more of the conversation history and the cost/latency that you'll incur by including those tokens in the prompt. Shorter prompts are cheaper and faster. The amount of the previous conversation you include also makes a difference in how the model responds.\n",
    "\n",
    "In this notebook, we'll show two strategies for managing the conversation history when working with the ChatGPT model.\n",
    "- Option 1: Keep the conversation within a given token limit\n",
    "- Option 2: Keep the conversation within a given number of turns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Keep the conversation within a given token limit\n",
    "\n",
    "`overall_max_tokens` is the maximum number of tokens that you want to include in the prompt. Th maximum number this can be set to is 4096 but you can also consider reducing this number to reduce the cost and latency of the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_max_tokens = 1096\n",
    "prompt_max_tokens = overall_max_tokens - max_response_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can continue the conversation below by editing the user_message and running the cell as many times as you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 175\n",
      "[USER]\n",
      "I want to write a blog post about the impact of AI on the future of work.\n",
      "\n",
      "[ASSISTANT]\n",
      "### The Impact of AI on the Future of Work\n",
      "\n",
      "As we stand on the brink of a technological revolution, the impact of artificial intelligence (AI) on the future of work is becoming increasingly apparent. From reshaping industries to changing job roles and even redefining the skills needed in the workforce, AI is set to transform the way we work in profound ways.\n",
      "\n",
      "#### The Automation of Routine Tasks\n",
      "\n",
      "One of the most significant impacts of AI in the workplace is the automation of routine and repetitive tasks. Many\n",
      "\n",
      "[USER]\n",
      "The target audience for the blog post should be business leaders working in the tech industry.\n",
      "\n",
      "[USER]\n",
      "The target audience for the blog post should be business leaders working in the tech industry.\n",
      "\n",
      "[ASSISTANT]\n",
      "### The Impact of AI on the Future of Work: Insights for Tech Industry Leaders\n",
      "\n",
      "As business leaders in the tech industry, you are at the forefront of innovations that are reshaping not just how we interact with technology but also how we approach work itself. The advent of artificial intelligence (AI) is one of the most transformative trends, set to redefine workplaces, industries, and the global economy. Understanding the various dimensions of AI's impact is crucial for strategic decision-making and sustainable growth.\n",
      "\n",
      "#### 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_message = \"The target audience for the blog post should be business leaders working in the tech industry.\"\n",
    "#user_message = \"Let's talk about generative AI and keep the tone informational but also friendly.\"\n",
    "#user_message = \"Show me a few more examples\"\n",
    "messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "token_count = num_tokens_from_messages(messages)\n",
    "print(f\"Token count: {token_count}\")\n",
    "\n",
    "# remove first message while over the token limit\n",
    "while token_count > prompt_max_tokens:\n",
    "    messages.pop(0)\n",
    "    token_count = num_tokens_from_messages(messages)\n",
    "\n",
    "response = send_message(messages, deployment_name, max_response_tokens)\n",
    "\n",
    "#Append assistance response \n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "print_conversation(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Keep the conversation within a given number of turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_messages = 10\n",
    "\n",
    "overall_max_tokens = 4096\n",
    "prompt_max_tokens = overall_max_tokens - max_response_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can continue the conversation below by editing the user_message and running the cell as many times as you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]\n",
      "Make the post about generative AI aimed at business leaders who have some knowledge of the technology.\n",
      "\n",
      "[USER]\n",
      "Make the post about generative AI aimed at business leaders who have some knowledge of the technology.\n",
      "\n",
      "[USER]\n",
      "Make the post about generative AI aimed at business leaders who have some knowledge of the technology.\n",
      "\n",
      "[USER]\n",
      "Make the post about generative AI aimed at business leaders who have some knowledge of the technology.\n",
      "\n",
      "[ASSISTANT]\n",
      "### Harnessing the Power of Generative AI: What Business Leaders Need to Know\n",
      "\n",
      "As business leaders, you are no strangers to the transformative potential of technology. Generative AI, in particular, is a frontier that promises to reshape industries, streamline operations, and drive innovation. With some foundational knowledge of the technology, now is the time to delve deeper into how generative AI can create tangible value for your organization.\n",
      "\n",
      "#### Understanding Generative AI\n",
      "\n",
      "At its core, generative AI refers to algorithms that\n",
      "\n",
      "[USER]\n",
      "cual a sido el dia mas caluroso de la hisotoria\n",
      "\n",
      "[ASSISTANT]\n",
      "El día más caluroso registrado en la historia se produjo el 10 de julio de 1913, en Furnace Creek Ranch, ubicado en el Valle de la Muerte, California, Estados Unidos. En ese día, se registró una temperatura de 56.7 °C (134 °F). Este récord ha permanecido como el más alto oficialmente documentado. \n",
      "\n",
      "Es importante señalar que ha habido discusiones sobre la validez de algunos récords anteriores y sobre las mediciones de temperatura\n",
      "\n",
      "[USER]\n",
      "cual a sido el dia mas caluroso de la hisotoria\n",
      "\n",
      "[ASSISTANT]\n",
      "El día más caluroso registrado en la historia tuvo lugar el 10 de julio de 1913 en Furnace Creek Ranch, en el Valle de la Muerte, California, Estados Unidos, donde se registró una temperatura de 56.7 °C (134 °F). Este récord sigue siendo el más alto oficialmente reconocido por la Organización Meteorológica Mundial. \n",
      "\n",
      "Si bien hay otros días con temperaturas extremadamente altas, este sigue siendo el récord más antiguo y documentado en condiciones estándar.\n",
      "\n",
      "[USER]\n",
      "Make the post about generative AI aimed at business leaders who have some knowledge of the technology.\n",
      "\n",
      "[ASSISTANT]\n",
      "### Generative AI: Unlocking New Opportunities for Business Leaders\n",
      "\n",
      "As business leaders navigating a rapidly evolving technological landscape, understanding and leveraging generative AI can be pivotal for driving innovation and maintaining competitive advantage. With a foundational knowledge already in place, let’s explore the transformative potential of generative AI and how it can be integrated into your strategic initiatives.\n",
      "\n",
      "#### What is Generative AI?\n",
      "\n",
      "Generative AI refers to algorithms that can generate new content—be it text, images, music, or even code—\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Make the post about generative AI aimed at business leaders who have some knowledge of the technology.\"\n",
    "messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "while len(messages) > max_messages:\n",
    "    messages.pop(0)\n",
    "\n",
    "token_count = num_tokens_from_messages(messages)\n",
    "\n",
    "while token_count > prompt_max_tokens:\n",
    "    # remove first message from messages\n",
    "    messages.pop(0)\n",
    "    token_count = num_tokens_from_messages(messages)\n",
    "\n",
    "response = send_message(messages, deployment_name, max_response_tokens)\n",
    "\n",
    "#Append assistant response\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "print_conversation(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Modificar el Mensaje del Sistema\n",
    "Enunciado:\n",
    "Crea un mensaje del sistema que defina a ChatGPT como un \"analista financiero experto\". Imprime el mensaje del sistema resultante y utilízalo en un mensaje inicial del usuario que pregunte:\n",
    "\"¿Cuáles son las tendencias actuales en el mercado de acciones tecnológicas?\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a financial analyst expert.\n",
      "[USER]\n",
      "que vision de futuro ves a las cryptomonedas\n",
      "\n",
      "[ASSISTANT]\n",
      "La visión de futuro para las criptomonedas es compleja y multifacética. Aquí hay algunos aspectos clave que podrían definir su evolución:\n",
      "\n",
      "1. **Adopción Generalizada**: Es probable que las criptomonedas continúen ganando aceptación tanto por individuos como por instituciones. Esto podría incluir su uso como medio de intercambio, reserva de valor y en contratos inteligentes.\n",
      "\n",
      "2. **Regulación**: A medida que la industria madura, los gobiernos y las autoridades financieras probablemente aumentarán la\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mensaje del sistema\n",
    "base_system_message = \"You are a financial analyst expert.\"\n",
    "system_message = f\"{base_system_message.strip()}\"\n",
    "print(system_message)\n",
    "\n",
    "# Crear lista de mensajes\n",
    "user_message = \"que vision de futuro ves a las cryptomonedas\" \n",
    "messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "\n",
    "# Enviar mensaje y recibir respuesta\n",
    "response = send_message(messages, deployment_name, max_response_tokens)\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "print_conversation(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Contar Tokens en una Conversación Extendida\n",
    "Enunciado:\n",
    "Dado un conjunto inicial de mensajes, agrega tres mensajes adicionales:\n",
    "\n",
    "\"Explain the impact of inflation on stock prices.\"\n",
    "\"What are the top stocks to watch in 2025?\"\n",
    "\"Summarize the latest market news in 50 words.\"\n",
    "Calcula y muestra el número total de tokens tras añadir cada mensaje.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count after adding 'Explain the impact of inflation on stock prices.': 48\n",
      "Token count after adding 'What are the top stocks to watch in 2025?': 65\n",
      "Token count after adding 'Summarize the latest market news in 50 words.': 82\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mensajes iniciales\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in financial topics.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the current trends in the tech stock market?\"}\n",
    "]\n",
    "\n",
    "# Mensajes adicionales\n",
    "additional_messages = [\n",
    "    \"Explain the impact of inflation on stock prices.\",\n",
    "    \"What are the top stocks to watch in 2025?\",\n",
    "    \"Summarize the latest market news in 50 words.\"\n",
    "]\n",
    "\n",
    "# Agregar mensajes y calcular tokens\n",
    "for msg in additional_messages:\n",
    "    messages.append({\"role\": \"user\", \"content\": msg})\n",
    "    token_count = num_tokens_from_messages(messages)\n",
    "    print(f\"Token count after adding '{msg}': {token_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Personalizar el Mensaje del Sistema Según el Usuario\n",
    "Enunciado:\n",
    "Escribe un programa que permita al usuario definir el rol de ChatGPT mediante una entrada de texto (input). Una vez definido, responde al mensaje:\n",
    "\"What are your key skills as an assistant?\"\n",
    "Usa el mensaje del sistema definido por el usuario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYSTEM]\n",
      "You are eres un asistente de matematicas.\n",
      "\n",
      "[USER]\n",
      "What are your key skills as an assistant?\n",
      "\n",
      "[ASSISTANT]\n",
      "As an assistant focused on mathematics, my key skills include:\n",
      "\n",
      "1. **Problem Solving**: I can help you solve a wide range of mathematical problems, from basic arithmetic to advanced calculus and statistics.\n",
      "\n",
      "2. **Concept Explanation**: I can explain mathematical concepts and theories in a clear and understandable manner.\n",
      "\n",
      "3. **Step-by-Step Guidance**: I can provide step-by-step solutions to problems, helping you understand the process involved.\n",
      "\n",
      "4. **Formula Application**: I can assist with the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Entrada del usuario para definir el mensaje del sistema\n",
    "user_defined_role = input(\"Define the role of the assistant: \")\n",
    "system_message = f\"You are {user_defined_role.strip()}.\" #Se puede escribir el mensaje arriba del visual\n",
    "\n",
    "# Crear lista de mensajes\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": \"What are your key skills as an assistant?\"}\n",
    "]\n",
    "\n",
    "# Enviar mensaje y recibir respuesta\n",
    "response = send_message(messages, deployment_name, max_response_tokens)\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Mostrar la conversación\n",
    "print_conversation(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4: Crear un Generador de Respuestas Creativo\n",
    "Enunciado:\n",
    "Diseña un programa que permita configurar la creatividad del modelo (temperatura) a través de una entrada del usuario. Una vez configurado, responde a la pregunta:\n",
    "\"Can you suggest an out-of-the-box idea for a startup?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Claro! Aquí tienes una idea de startup:\n",
      "\n",
      "**Plataforma de Intercambio de Habilidades Locales**\n",
      "\n",
      "**Descripción:** Crea una plataforma que conecte a personas en una comunidad local que deseen intercambiar habilidades. Por ejemplo, alguien que sepa tocar la guitarra podría ofrecer clases a cambio de lecciones de cocina de otra persona. La idea es fomentar el aprendizaje y la colaboración entre vecinos, promoviendo un sentido de comunidad y reduciendo la necesidad de dinero en transacciones.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Entrada del usuario para definir la creatividad\n",
    "temperature = float(input(\"Set the creativity level (0.0 - 1.0): \"))\n",
    "if not (0.0 <= temperature <= 1.0):\n",
    "    raise ValueError(\"Creativity level must be between 0.0 and 1.0.\")\n",
    "\n",
    "# Crear lista de mensajes\n",
    "system_message = \"You are a highly creative assistant.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": \"me puedes dar una idea de startup\"}\n",
    "]\n",
    "\n",
    "# Enviar mensaje con creatividad ajustada\n",
    "response = send_message(messages, deployment_name, max_response_tokens, temperature)\n",
    "\n",
    "#Mostrar la respuesta\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 5: Cambiar el Tono del Mensaje\n",
    "Enunciado:\n",
    "Configura el mensaje del sistema para que ChatGPT responda en un tono humorístico y escribe una respuesta al mensaje:\n",
    "\"Explain quantum computing in a funny way.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYSTEM]\n",
      "You are a humorous assistant.\n",
      "\n",
      "[USER]\n",
      "Explain quantum computing in a funny way.\n",
      "\n",
      "[ASSISTANT]\n",
      "Sure! Imagine you're trying to find your car keys. In the classical world, you’re like an annoyed detective who methodically searches every inch of your house, checking under couch cushions and inside the fridge (because, let’s be honest, you’re hungry and it’s possible they fell in there during a snack break).\n",
      "\n",
      "Now, enter the quantum world! It's like you suddenly have access to a superpower where you can actually be in two places at once. You could simultaneously look in the fridge and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mensaje del sistema humorístico\n",
    "system_message = \"You are a humorous assistant.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing in a funny way.\"}\n",
    "]\n",
    "\n",
    "# Enviar mensaje y recibir respuesta\n",
    "response = send_message(messages, deployment_name, max_response_tokens)\n",
    "\n",
    "#append assistant response\n",
    "messages.append ({\"role\": \"assistant\", \"content\": response})\n",
    "print_conversation(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 6: Resumir Mensajes Previos\n",
    "Enunciado:\n",
    "Dado un historial de conversación de 5 mensajes, crea una función que resuma el contenido de los mensajes del usuario en una sola oración.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User messages summary: AI encompasses machine learning, which uses algorithms to enable computers to learn from data; neural networks, a key architecture in this field, mimic the way the human brain works; and deep learning, a subset of machine learning, has numerous applications including image recognition, natural language processing, and autonomous systems.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Historial de mensajes\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about AI.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How does machine learning work?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you explain neural networks?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the applications of deep learning?\"}\n",
    "]\n",
    "\n",
    "# Resumir mensajes del usuario\n",
    "def summarize_user_messages(messages):\n",
    "    user_messages = [msg['content'] for msg in messages if msg['role'] == 'user']\n",
    "    summary_prompt = \"Summarize the following messages into one sentence: \" + \" \".join(user_messages)\n",
    "    summary = send_message([{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                            {\"role\": \"user\", \"content\": summary_prompt}], deployment_name)\n",
    "    return summary\n",
    "\n",
    "# Obtener y mostrar el resumen\n",
    "summary = summarize_user_messages(messages)\n",
    "print(f\"User messages summary: {summary}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 7: Configuración de Temperatura y Tono\n",
    "Enunciado:\n",
    "Escribe un programa que cambie la configuración de temperatura a 0.9 para obtener respuestas más creativas y responde al mensaje:\n",
    "\"Suggest a unique title for a blog about AI in education.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"EduTech Evolution: Harnessing AI for Tomorrow's Classrooms\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mensajes iniciales\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a creative assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Suggest a unique title for a blog about AI in education.\"}\n",
    "]\n",
    "\n",
    "# Ajustar temperatura y enviar mensaje\n",
    "response = send_message(messages, deployment_name, max_response_tokens,temperature=0.9)\n",
    "\n",
    "# Mostrar la respuesta\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
